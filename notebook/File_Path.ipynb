{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a702b1e-5ece-4ca6-a2fd-601dd7ab5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import Feature_Extraction\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f023bfa-a386-49db-a05e-7706b8e73feb",
   "metadata": {},
   "source": [
    "# File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59802e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.read_csv('Dataset/File Path/full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0394c0-35bf-4490-b9aa-614ffb7c4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vectors(train_data, path=None):\n",
    "    # Extracting sentences from the 'process_path' column in the provided DataFrame\n",
    "    sentences = train_data['process_path'].str.lower().str.split('\\\\').tolist()\n",
    "    \n",
    "    # Creating a FastText model with specific parameters\n",
    "    model = FastText(\n",
    "        vector_size=32,      # Dimensionality of the word vectors\n",
    "        window=10,           # Maximum distance between the current and predicted word within a sentence\n",
    "        min_count=1,         # Ignores words with total frequency lower than this\n",
    "        workers=4,           # Number of CPU cores to use\n",
    "        sg=0,                # Skip-gram model (sg=0) or CBOW model (sg=1)\n",
    "        sentences=sentences  # Training sentences\n",
    "    )\n",
    "    \n",
    "    # Training the FastText model\n",
    "    model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "    \n",
    "    # Saving the trained word vectors to a specified path if provided\n",
    "    if path is not None:\n",
    "        model.wv.save(path)\n",
    "    \n",
    "    # Returning the word vectors\n",
    "    return model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb1a37e-d229-4ea8-9a48-61917a837242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(path):\n",
    "    return KeyedVectors.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5219a7a0-3b89-4c62-924d-eb145f246529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assuming 'load_word_vectors' and 'build_word_vectors' functions are defined\n",
    "\n",
    "WORD_VECTORS_PATH = 'Model/File Path/Word Vector/fasttext_v1.model'\n",
    "\n",
    "# Check if the file containing word vectors exists\n",
    "if os.path.isfile(WORD_VECTORS_PATH):\n",
    "    # Load word vectors if the file exists\n",
    "    word_vectors = load_word_vectors(WORD_VECTORS_PATH)\n",
    "    print(1)\n",
    "else:\n",
    "    # Build word vectors if the file does not exist\n",
    "    word_vectors = build_word_vectors(file_df, WORD_VECTORS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff0f977-2d6b-404f-a298-bc7c59f48c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'word_vectors' and 'file_df' are defined\n",
    "\n",
    "# Tokenize the paths in the 'process_path' column and convert to lowercase\n",
    "file_df['text_tokenized'] = file_df['process_path'].str.lower().str.split('\\\\')\n",
    "\n",
    "# Calculate the mean vector for each tokenized path using pre-trained word vectors\n",
    "file_df['text_vect_mean'] = file_df['text_tokenized'].apply(\n",
    "    lambda x: np.array([word_vectors[token] for token in x]).mean(axis=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dfb9959-2f05-4c5f-bada-c4b8bdb73891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Return a dictionary containing the calculated metrics\n",
    "    return {'accuracy': accuracy, 'precision': precision,\n",
    "            'recall': recall, 'f1_score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4ebb30-3a87-45f4-8863-9d2c412497b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def get_model(cfg, name='xgb'):\n",
    "    \"\"\"\n",
    "    Get a machine learning model based on the specified name and configuration.\n",
    "\n",
    "    Parameters:\n",
    "    - cfg (dict): Configuration parameters for the model.\n",
    "    - name (str): Name of the model (default is 'xgb').\n",
    "\n",
    "    Returns:\n",
    "    - model: An instance of a machine learning model.\n",
    "    \"\"\"\n",
    "    if name == 'rf':\n",
    "        # Random Forest Classifier\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=cfg['n_estimators'], \n",
    "            max_depth=cfg['max_depth'], \n",
    "            min_samples_split=cfg['min_samples_split'], \n",
    "            min_samples_leaf=cfg['min_samples_leaf'], \n",
    "            max_features=cfg['max_features'], \n",
    "            n_jobs=-1, \n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "    elif name == 'dt':\n",
    "        # Decision Tree Classifier\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=cfg['max_depth'], \n",
    "            min_samples_split=cfg['min_samples_split'], \n",
    "            min_samples_leaf=cfg['min_samples_leaf'], \n",
    "            max_features=cfg['max_features'],\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        # Default: XGBoost or another model\n",
    "        # Add additional model implementations as needed\n",
    "        raise ValueError(f\"Unsupported model name: {name}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d911037-de7d-4c56-b0ad-18d79e6e5a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9882014823778551,\n",
       " 'precision': 0.9337620578778135,\n",
       " 'recall': 0.9647840531561461,\n",
       " 'f1_score': 0.9490196078431372}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration parameters for the Random Forest model\n",
    "RF_CONFIG = dict(\n",
    "    n_estimators=60,\n",
    "    max_depth=34,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=0.65\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(file_df, random_state=42, stratify=file_df['label'], test_size=0.2)\n",
    "\n",
    "# Get a Random Forest model using the specified configuration\n",
    "rf_model = get_model(RF_CONFIG, name='rf')\n",
    "\n",
    "# Train the Random Forest model on the training data\n",
    "rf_model.fit(np.stack(train_data['text_vect_mean']), train_data['label'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "rf_pred = rf_model.predict(np.stack(test_data['text_vect_mean']))\n",
    "\n",
    "# Calculate performance scores using the 'score' function\n",
    "rf_scores = score(test_data['label'], rf_pred)\n",
    "\n",
    "# Print or use the calculated scores as needed\n",
    "print(rf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c2f981-afb6-4777-84a6-6c192b5093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a Random Forest model with the specified configuration\n",
    "rf_model = get_model(RF_CONFIG, name='rf')\n",
    "\n",
    "# Train the Random Forest model on the entire dataset\n",
    "rf_model.fit(np.stack(file_df['text_vect_mean']), file_df['label'])\n",
    "\n",
    "# Specify the filename to save the trained model\n",
    "filename = 'Model/File Path/rf_model.sav'\n",
    "\n",
    "# Save the trained Random Forest model to the specified file using pickle\n",
    "pickle.dump(rf_model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e186d3f3-8ac8-4f33-8978-b7f689ddcc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9847980638330056,\n",
       " 'precision': 0.9617563739376771,\n",
       " 'recall': 0.9023255813953488,\n",
       " 'f1_score': 0.9310935893040795}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration parameters for the Decision Tree model\n",
    "DT_CONFIG = dict(\n",
    "    max_depth=6,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='log2'\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(file_df, random_state=42, stratify=file_df['label'], test_size=0.2)\n",
    "\n",
    "# Get a Decision Tree model using the specified configuration\n",
    "dt_model = get_model(DT_CONFIG, name='dt')\n",
    "\n",
    "# Train the Decision Tree model on the training data\n",
    "dt_model.fit(np.stack(train_data['text_vect_mean']), train_data['label'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "dt_pred = dt_model.predict(np.stack(test_data['text_vect_mean']))\n",
    "\n",
    "# Calculate performance scores using the 'score' function\n",
    "dt_scores = score(test_data['label'], dt_pred)\n",
    "\n",
    "# Print or use the calculated scores as needed\n",
    "print(dt_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2507410-11bb-4d68-8a84-eaea9be3e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Decision Tree model using the specified configuration\n",
    "dt_model = get_model(DT_CONFIG, name='dt')\n",
    "\n",
    "# Train the Decision Tree model on the entire dataset\n",
    "dt_model.fit(np.stack(file_df['text_vect_mean']), file_df['label'])\n",
    "\n",
    "# Specify the filename to save the trained model\n",
    "filename = 'Model/File Path/dt_model.sav'\n",
    "\n",
    "# Save the trained Decision Tree model to the specified file using pickle\n",
    "pickle.dump(dt_model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc463941-0117-4506-8561-57020d32707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the paths in the 'process_path' column and create a new column 'text_tokenized'\n",
    "file_phish['text_tokenized'] = file_phish['process_path'].str.lower().str.split('\\\\')\n",
    "\n",
    "# Calculate the mean vector representation for each path and create a new column 'text_vect_mean'\n",
    "file_phish['text_vect_mean'] = file_phish['text_tokenized'].apply(\n",
    "    lambda x: np.array([word_vectors[token] for token in x]).mean(axis=0)\n",
    ")\n",
    "\n",
    "# Use the pre-trained Random Forest model to predict labels for the new dataset\n",
    "tmp_pred = rf_model.predict(np.stack(file_phish['text_vect_mean']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e35e80-6057-4d03-818c-064acf4c2b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame 'x' by stacking the 'text_vect_mean' column\n",
    "x = pd.DataFrame(np.stack(file_df['text_vect_mean']))\n",
    "\n",
    "# Generate column names 'd_0', 'd_1', ..., 'd_299'\n",
    "cols = [f'd_{i}' for i in range(300)]\n",
    "\n",
    "# Assign the generated column names to the columns of DataFrame 'x'\n",
    "x.columns = cols\n",
    "\n",
    "# Add a 'Label' column to 'x' by resetting the index of the 'label' column from 'file_df'\n",
    "x['Label'] = file_df['label'].reset_index(drop=True)\n",
    "\n",
    "# Save the DataFrame 'x' to a CSV file named 'training_final.csv' without including the index column\n",
    "x.to_csv('Dataset/File Path/training_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
